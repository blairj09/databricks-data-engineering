---
title: "Data Engineering with Positron + Databricks"
subtitle: "Why Your IDE Matters for Production Pipelines"
format:
  revealjs:
    theme: [default, custom.scss]
    slide-number: true
    footer: "Data Engineering with Positron + Databricks"
    logo: img/posit-logo.svg
    transition: fade
    highlight-style: github
    code-line-numbers: false
---

## {background-color="#447099"}

::: {.agenda-nav}
[**Set the Stage**]{.active}

[**Positron + Databricks**]{.active}

[**Live Demo**]{.active}

[**IDE vs Notebooks**]{.active}

[**Q&A**]{.active}
:::

::: {.notes}
~45-50 minutes total. Demo is the core - about 30 minutes.
:::

## {background-color="#447099" visibility="uncounted"}

::: {.agenda-nav}
[**Set the Stage**]{.active}

[Positron + Databricks]{.faded}

[Live Demo]{.faded}

[IDE vs Notebooks]{.faded}

[Q&A]{.faded}
:::

## The Business Question

> "How does weather impact NYC taxi operations?"

- Do rainy days increase ridership?
- Does extreme cold/heat affect fares?
- Can we predict demand based on weather forecasts?

::: {.notes}
Set up the business context. This is a realistic analytics question that requires data integration.
:::

## Current State: Taxi Data in Databricks

We already have NYC taxi trip data in Unity Catalog:

- **samples.nyctaxi.trips** - Trip records with pickup/dropoff times, fares, locations
- Millions of records
- No weather context

::: {.fragment}
**The gap**: We can see *what* happened, but not *why*.
:::

::: {.notes}
Show Catalog Explorer here if time permits - browse the existing taxi data.
:::

## What We Need

Integrate **historical weather data** with taxi trips:

- Temperature, precipitation, wind speed
- Hourly granularity to match trip timestamps
- Join on date/time to enable weather-based analysis

::: {.fragment}
Let's build this pipeline - the Data Engineering way.
:::

## {background-color="#447099" visibility="uncounted"}

::: {.agenda-nav}
[Set the Stage]{.faded}

[**Positron + Databricks**]{.active}

[Live Demo]{.faded}

[IDE vs Notebooks]{.faded}

[Q&A]{.faded}
:::

## What is Positron?

A next-generation IDE from Posit (makers of RStudio)

- Built on VS Code
- Designed for **data work** - R, Python, and more
- Native support for Databricks workflows via Databricks VS Code Extension

## {background-color="#447099" visibility="uncounted"}

::: {.agenda-nav}
[Set the Stage]{.faded}

[Positron + Databricks]{.faded}

[**Live Demo**]{.active}

[IDE vs Notebooks]{.faded}

[Q&A]{.faded}
:::

## Demo: Building a Weather Pipeline

We'll walk through the full Data Engineering workflow:

1. **Identify** new data (Open-Meteo API)
2. **Explore** data structure and quality
3. **Integrate** with existing taxi data
4. **Deploy** with Databricks Asset Bundles

::: {.notes}
Switch to Positron for the demo. Keep slides minimal from here - the screen should show the IDE.
:::

## {background-color="#447099" visibility="uncounted"}

::: {.agenda-nav}
[Set the Stage]{.faded}

[Positron + Databricks]{.faded}

[Live Demo]{.faded}

[**IDE vs Notebooks**]{.active}

[Q&A]{.faded}
:::

## Notebook Pain Points {.smaller}

| Notebook | IDE |
|---------------------|--------------|
| Messy git diffs | Standard Python files, clean diffs |
| Copy-paste code between notebooks | Proper imports and shared modules |
| Difficult code review | PR-based workflows with real diffs |
| Challenging unit tests | pytest with mocked Spark |
| Lack of debugging tools | Breakpoints, stack traces, debugger |
| Hidden state from cell execution order | Stateless functions, explicit dependencies |

## Code Review

::: {.columns}
::: {.column width="50%"}
**Notebook PR Diff**

```json
{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 500 lines of JSON..."
      ]
    }
  ]
}
```
:::

::: {.column width="50%"}
**Python File Diff**

```diff
- def join_weather(trips_df):
+ def join_weather(trips_df, weather_df):
      return trips_df.join(
-         spark.table("weather"),
+         weather_df,
          on="pickup_date"
      )
```
:::
:::

## CI/CD Integration

The same tests run everywhere:

```bash
# Local development
pytest tests/

# GitHub Actions (on every PR)
pytest tests/

# Pre-deployment validation
databricks bundle validate && pytest tests/
```

::: {.fragment}
**One codebase. One test suite. Multiple environments.**
:::

## When Notebooks Work Best

::: {.incremental}
- **Exploration**: Ad-hoc data analysis
- **Prototyping**: Quick experiments
- **Sharing**: Results with stakeholders
- **Teaching**: Interactive tutorials
:::

::: {.fragment}
**The key**: Production pipelines deserve production tooling.
:::

## Key Takeaways

::: {.incremental}
1. **Positron** brings professional IDE features to data work
2. **Databricks Asset Bundles** enable infrastructure as code
3. **Lakeflow Declarative Pipelines** simplify pipeline development
4. **IDE workflows** improve reliability, testing, and collaboration
5. **Use the right tool** - notebooks for exploration, IDE for production
:::

## {background-color="#447099" visibility="uncounted"}

::: {.agenda-nav}
[Set the Stage]{.faded}

[Positron + Databricks]{.faded}

[Live Demo]{.faded}

[IDE vs Notebooks]{.faded}

[**Q&A**]{.active}
:::

## Resources

- **This repo**: [github.com/blairj09/databricks-data-engineering](https://github.com/blairj09/databricks-data-engineering)
- **Positron**: [positron.posit.co](https://positron.posit.co)
- **Databricks Asset Bundles**: [docs.databricks.com/dev-tools/bundles](https://docs.databricks.com/dev-tools/bundles)
- **Lakeflow Declarative Pipelines**: [docs.databricks.com/aws/en/ldp](https://docs.databricks.com/aws/en/ldp/)

::: {.notes}
Thank you! Questions?
:::
